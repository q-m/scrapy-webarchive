{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scrapy Webarchive","text":"<p><code>scrapy-webarchive</code> is a plugin for Scrapy that allows users to capture and export web archives in the WARC and WACZ formats during crawling.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Save web crawls in WACZ format (multiple storages supported; local and cloud).</li> <li>Crawl against WACZ format archives.</li> <li>Integrate seamlessly with Scrapy\u2019s spider request and response cycle.</li> </ul>"},{"location":"#limitations","title":"Limitations","text":"<ul> <li>WACZ supports saving images but this module does not yet integrate with Scrapy's image/file pipeline for retrieving images/files from the WACZ. Future support for this feature is planned.</li> </ul> <p>Source Code: https://github.com/q-m/scrapy-webarchive</p>"},{"location":"#credits","title":"Credits","text":"<p>This package started as a fork of https://github.com/internetarchive/scrapy-warcio. The idea of turning its functionality into an extension as well as actually writing the WARC files is based on it.</p>"},{"location":"advanced_usage/","title":"Advanced usage","text":""},{"location":"advanced_usage/#crawling","title":"Crawling","text":""},{"location":"advanced_usage/#skipping-specific-requests","title":"Skipping specific requests","text":"<p>The <code>wacz_crawl_skip</code> flag is applied to requests that should be ignored by the crawler. When this flag is present, the middleware intercepts the request and prevents it from being processed further, skipping both download and parsing. This is useful in scenarios where the request should not be collected during a scraping session. Usage:</p> <pre><code>yield Request(url, callback=cb_func, flags=[\"wacz_crawl_skip\"])\n</code></pre> <p>When this happens, the statistic <code>webarchive/crawl_skip</code> is increased.</p>"},{"location":"advanced_usage/#disallowing-archived-urls","title":"Disallowing archived URLs","text":"<p>If the spider has the attribute <code>archive_disallow_regexp</code>, all requests returned from the spider that match this regular expression, are ignored. For example, when a product page was returned in <code>start_requests</code>, but the product page disappeared and redirected to its category page, the category page can be disallowed, so as to avoid crawling the whole category, which would take much more time and could lead to unknown URLs (e.g. the spider's requested pagination size could be different from the website default).</p> <p>When this happens, the statistic <code>wacz/crawl_skip/disallowed</code> is increased.</p>"},{"location":"advanced_usage/#iterating-a-wacz-archive-index","title":"Iterating a WACZ archive index","text":"<p>When using a WACZ file that is not generated by your own spiders, it might be that the spider for crawling is not in place. In order to crawl this WACZ you need to tailor a spider to work with this specific WACZ file. This will require building the spider different to what it is supposed to look like with a live resource.</p> <p>Going around the default behaviour of the spider, the <code>WaczCrawlMiddleware</code> spider middleware will, when enabled, replace the crawl by an iteration through all the entries in the WACZ archive index.</p>"},{"location":"advanced_usage/#configuration","title":"Configuration","text":"<p>To use this strategy, enable both the spider- and the downloadermiddleware in the spider settings like so:</p> settings.py<pre><code>DOWNLOADER_MIDDLEWARES = {\n    \"scrapy_webarchive.downloadermiddlewares.WaczMiddleware\": 543,\n}\n\nSPIDER_MIDDLEWARES = {\n    \"scrapy_webarchive.spidermiddlewares.WaczCrawlMiddleware\": 543,\n}\n</code></pre> <p>Then define the location of the WACZ archive with <code>SW_WACZ_SOURCE_URI</code> setting:</p> settings.py<pre><code>SW_WACZ_SOURCE_URI = \"s3://scrapy-webarchive/archive.wacz\"\nSW_WACZ_CRAWL = True\n</code></pre>"},{"location":"advanced_usage/#controlling-the-crawl","title":"Controlling the crawl","text":"<p>Not all URLs will be interesting for the crawl since your WACZ will most likely contain static files such as fonts, JavaScript (website and external), stylesheets, etc. In order to improve the performance of the spider by not reading all the irrelevant request/response entries, you can configure the following atrribute in your spider, <code>archive_regex</code>:</p> my_wacz_spider.py<pre><code>from scrapy.spiders import Spider\n\n\nclass MyWaczSpider(Spider):\n    name = \"myspider\"\n    archive_regex = r\"^/tag/[\\w-]+/$\"\n</code></pre> <p>If the spider has an <code>archive_regexp</code> attribute, only response URLs matching this regexp are presented in <code>start_requests</code>. To visualise that, the spider above will only crawl the indented cdxj records below:</p> <pre><code>com,toscrape,quotes)/favicon.ico 20241007081411465 {...}\ncom,gstatic,fonts)/s/raleway/v34/1ptug8zys_skggpnyc0it4ttdfa.woff2 {...}\ncom,googleapis,fonts)/css?family=raleway%3A400%2C700 20241007081525229 {...}\ncom,toscrape,quotes)/static/bootstrap.min.css 20241007081525202 {...}\ncom,toscrape,quotes)/static/main.css 20241007081525074 {...}\n&gt; com,toscrape,quotes)/tag/books/ 20241007081513898 {...}\n&gt; com,toscrape,quotes)/tag/friends/ 20241007081520928 {...}\n&gt; com,toscrape,quotes)/tag/friendship/ 20241007081519648 {...}\n&gt; com,toscrape,quotes)/tag/humor/ 20241007081512594 {...}\n&gt; com,toscrape,quotes)/tag/inspirational/ 20241007081506990 {...}\n&gt; com,toscrape,quotes)/tag/life/ 20241007081510349 {...}\n&gt; com,toscrape,quotes)/tag/love/ 20241007081503814 {...}\n&gt; com,toscrape,quotes)/tag/reading/ 20241007081516781 {...}\n&gt; com,toscrape,quotes)/tag/simile/ 20241007081524944 {...}\n&gt; com,toscrape,quotes)/tag/truth/ 20241007081523804 {...}\n</code></pre>"},{"location":"advanced_usage/#requests-and-responses","title":"Requests and Responses","text":""},{"location":"advanced_usage/#special-keys-in-requestmeta","title":"Special Keys in Request.meta","text":"<p>The <code>Request.meta</code> attribute in Scrapy allows you to store arbitrary data for use during the crawling process. While you can store any custom data in this attribute, Scrapy and its built-in extensions recognize certain special keys. Additionally, the <code>scrapy-webarchive</code> extension introduces its own special key for managing metadata. Below is a description of the key used by <code>scrapy-webarchive</code>:</p> <ul> <li><code>webarchive_warc</code></li> </ul>"},{"location":"advanced_usage/#webarchive_warc","title":"<code>webarchive_warc</code>","text":"<p>This key stores the result of a WACZ crawl or export. The data associated with this key is read-only and is not used to control Scrapy's behavior. The value of this key can be accessed using the constant <code>WEBARCHIVE_META_KEY</code>, but direct usage of this constant is discouraged. Instead, you should use the provided class method to instantiate a metadata object, as shown in the example below:</p> my_wacz_spider.py<pre><code>from scrapy.spiders import Spider\nfrom scrapy_webarchive.models import WarcMetadata\n\n\nclass MyWaczSpider(Spider):\n    name = \"myspider\"\n\n    def parse_function(self, response):\n        # Instantiate a WarcMetadata object from the response\n        warc_meta = WarcMetadata.from_response(response)\n\n        # Extract the attributes to attach while parsing a page/item\n        if warc_meta:\n            yield {\n                'warc_record_id': warc_meta.record_id,\n                'wacz_uri': warc_meta.wacz_uri,\n            }\n</code></pre>"},{"location":"advanced_usage/#extending","title":"Extending","text":""},{"location":"advanced_usage/#custom-strategies","title":"Custom strategies","text":"<p>You can extend <code>scrapy-webarchive</code> by adding your own custom file lookup strategy. This allows you to define a custom way to select files based on available file information (URI and last modified timestamp).</p> <p>To create a new strategy, define a class that implements the <code>FileLookupStrategy</code> interface and register it with <code>StrategyRegistry</code>.</p> strategies.py<pre><code>from typing import List, Optional\n\nfrom scrapy_webarchive.models import FileInfo\nfrom scrapy_webarchive.strategies import StrategyRegistry\n\n\n@StrategyRegistry.register(\"custom\")\nclass CustomStrategy:\n    def find(self, files: List[FileInfo], target_time: float) -&gt; Optional[str]:\n        # Logic goes here, should return a single URI or None. \n        # For examples see scrapy_webarchive.strategies.\n</code></pre> <p>Once registered, you can use your strategy by setting <code>SW_WACZ_LOOKUP_STRATEGY</code> in your Scrapy settings:</p> settings.py<pre><code>SW_WACZ_LOOKUP_STRATEGY = \"custom\"\n</code></pre> <p>If you're defining it inside a <code>strategies.py</code> module within your Scrapy project, it will be automatically discovered. Alternatively, you can imported it somewhere in your project, such as in your <code>settings.py</code> or <code>middlewares.py</code>:</p> settings.py<pre><code>import my_project.custom_strategies\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>To install <code>scrapy-webarchive</code>, run:</p> <pre><code>pip install scrapy-webarchive\n</code></pre> <p>If you want to use a cloud provider for storing/scraping, you opt-in to install these dependencies:</p> <pre><code>pip install scrapy-webarchive[aws]\n</code></pre> <pre><code>pip install scrapy-webarchive[gcs]\n</code></pre> <pre><code>pip install scrapy-webarchive[all]\n</code></pre>"},{"location":"settings/","title":"Settings","text":"<p><code>scrapy-webarchive</code> makes use of the following settings, in addition to Scrapy's settings. Note that all the settings are prefixed with <code>SW_</code>.</p>"},{"location":"settings/#extensions","title":"Extensions","text":""},{"location":"settings/#sw_export_uri","title":"<code>SW_EXPORT_URI</code>","text":"settings.py<pre><code># Either configure the directory where the output should be uploaded to\nSW_EXPORT_URI = \"s3://scrapy-webarchive/\"\nSW_EXPORT_URI = \"s3://scrapy-webarchive/{spider}/\"\nSW_EXPORT_URI = \"s3://scrapy-webarchive/{year}/{month}/{day}/{spider}/\"\n\n# OR add the file name for full control of the output\nSW_EXPORT_URI = \"s3://scrapy-webarchive/output.wacz\"\nSW_EXPORT_URI = \"s3://scrapy-webarchive/{spider}/output-{timestamp}.wacz\"\nSW_EXPORT_URI = \"s3://scrapy-webarchive/{year}/{month}/{day}/{spider}-{timestamp}.wacz\"\n\n# Local (No scheme assumes it is \"file://\")\nSW_EXPORT_URI = \"file:///path/to/output/{spider}/\"\nSW_EXPORT_URI = \"/path/to/output/{spider}/\"\n</code></pre> <p>This is the output path of the WACZ file. Multiple variables can be added that allow dynamic generation of the output path. </p> <p>Supported variables: <code>spider</code>, <code>year</code>, <code>month</code>, <code>day</code> and <code>timestamp</code>.</p>"},{"location":"settings/#sw_wacz_title","title":"<code>SW_WACZ_TITLE</code>","text":"<p>This setting defines the title of the WACZ used in the <code>datapackage.json</code>, which is generated durning the WACZ creation. It will default to the spider name if it is not configured.</p>"},{"location":"settings/#sw_wacz_description","title":"<code>SW_WACZ_DESCRIPTION</code>","text":"<p>This setting defines the description of the WACZ used in the <code>datapackage.json</code>, which is generated durning the WACZ creation. It will default to the spider name if it is not configured. Defaults to:</p> <p>This is the web archive generated by a scrapy-webarchive extension for the  spider. It is mainly for scraping purposes as it does not contain any js/css data. Though it can be replayed as bare HTML if the site does not depend on  JavaScript."},{"location":"settings/#downloader-middleware-and-spider-middleware","title":"Downloader middleware and spider middleware","text":""},{"location":"settings/#sw_wacz_source_uri","title":"<code>SW_WACZ_SOURCE_URI</code>","text":"<p>\u26a0\ufe0f Scraping against a remote source currently only supports AWS S3.</p> settings.py<pre><code># \"file://\" must be explicitly added, unlike SW_EXPORT_URI where it makes an assumption if no scheme is added.\nSW_WACZ_SOURCE_URI = \"file:///Users/username/Documents/archive.wacz\"\nSW_WACZ_SOURCE_URI = \"s3://scrapy-webarchive/archive.wacz\"\n\n# Allows multiple sources, comma seperated.\nSW_WACZ_SOURCE_URI = \"s3://scrapy-webarchive/archive.wacz,file:///Users/username/Documents/archive.wacz\"\n</code></pre> <p>This setting defines the location of the WACZ file that should be used as a source for the crawl job.</p>"},{"location":"settings/#sw_wacz_crawl","title":"<code>SW_WACZ_CRAWL</code>","text":"settings.py<pre><code>SW_WACZ_CRAWL = True\n</code></pre> <p>Setting to ignore original <code>start_requests</code>, just yield all responses found in WACZ. For more information see Iterating a WACZ archive index.</p>"},{"location":"settings/#sw_wacz_lookup_strategy","title":"<code>SW_WACZ_LOOKUP_STRATEGY</code>","text":"settings.py<pre><code>SW_WACZ_LOOKUP_STRATEGY = \"after\"\n</code></pre> <p>A setting that can be used in combination with <code>SW_EXPORT_URI</code> and <code>SW_WACZ_LOOKUP_TARGET</code> to automatically resolve the most relevant archive for scraping.</p> <p>Supported strategies include <code>after</code> and <code>before</code>. However, the implementation of custom strategies is also supported. See Custom strategies.</p>"},{"location":"settings/#sw_wacz_lookup_target","title":"<code>SW_WACZ_LOOKUP_TARGET</code>","text":"<p>This setting is used in combination with <code>SW_WACZ_LOOKUP_STRATEGY</code> to determine the most relevant archive for scraping based on the specified time. The value must be an ISO 8601 formatted timestamp (YYYY-MM-DDTHH:MM:SS), representing the preferred point in time for file selection.</p> settings.py<pre><code>SW_WACZ_LOOKUP_TARGET = \"2025-01-01T00:00:00\"\n</code></pre>"},{"location":"settings/#how-it-works","title":"How It Works","text":"<p>When <code>SW_EXPORT_URI</code> is set, <code>SW_WACZ_LOOKUP_TARGET</code> helps locate the closest matching file based on time. The strategy defined in <code>SW_WACZ_LOOKUP_STRATEGY</code> determines how files are selected relative to this timestamp. All of them are required in order to enable this feature.</p> <p>\u26a0\ufe0f When <code>SW_WACZ_SOURCE_URI</code> is set, these settings won't have any effect.</p>"},{"location":"settings/#example-scenarios","title":"Example Scenarios","text":"<ul> <li><code>before</code> strategy: Selects the file with the closest last modified date before or exactly at the given timestamp.</li> <li><code>after</code> strategy: Selects the file with the closest last modified date after or exactly at the given timestamp.</li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>The general use for this plugin is separated in two parts, exporting and crawling.</p> <ol> <li>Exporting; Run your spider with the extension to generate and export a WACZ file. This WACZ archive can be used in future crawls to retrieve historical data or simply to decrease the load on the website when your spider has changed but needs to run on the same data.</li> <li>Crawling; Re-run your spider on an WACZ archive that was generated previously. This time we will not be generating a new WACZ archive but simply retrieve each reponse from the WACZ instead of making a request to the live resource (website). The WACZ contains complete response data that will be reconstructed to actual <code>Response</code> objects.</li> </ol>"},{"location":"usage/#exporting","title":"Exporting","text":""},{"location":"usage/#exporting-a-wacz-archive","title":"Exporting a WACZ archive","text":"<p>To archive the requests/responses during a crawl job you need to enable the <code>WaczExporter</code> extension. </p> settings.py<pre><code>EXTENSIONS = {\n    \"scrapy_webarchive.extensions.WaczExporter\": 543,\n}\n</code></pre> <p>This extension also requires you to set the export location using the <code>SW_EXPORT_URI</code> settings (check the settings page for different options for exporting).</p> settings.py<pre><code>SW_EXPORT_URI = \"s3://scrapy-webarchive/\"\n</code></pre> <p>Running a crawl job using these settings will result in a newly created WACZ file on the specified output location.</p>"},{"location":"usage/#crawling","title":"Crawling","text":""},{"location":"usage/#using-the-download-middleware","title":"Using the download middleware","text":"<p>To crawl against a WACZ archive you need to use the <code>WaczMiddleware</code> downloader middleware. Instead of fetching the live resource the middleware will retrieve it from the archive and recreate a <code>Response</code> using the data from the archive.</p> <p>To use the downloader middleware, enable it in the settings like so:</p> settings.py<pre><code>DOWNLOADER_MIDDLEWARES = {\n    \"scrapy_webarchive.downloadermiddlewares.WaczMiddleware\": 543,\n}\n</code></pre> <p>Then define the location of the WACZ archive with <code>SW_WACZ_SOURCE_URI</code> setting:</p> settings.py<pre><code>SW_WACZ_SOURCE_URI = \"s3://scrapy-webarchive/archive.wacz\"\n</code></pre>"}]}